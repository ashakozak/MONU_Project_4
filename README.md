# MONU_Project_4
Final Project for Monash University Data Analytics Bootcamp

***

### Scope:

With the lines of truth being blurred between real and fake in the daily news, every day and people becoming more reliant on social media for their news sources - it is more important than ever to be able to differentiate the two for reliable and accurate new sources. In light of this, we have created a model to predict the likelihood that a news article is fake or real<br>

### TOOLS<br>
- MongoDB
- Python
- pandas
- matplotlib
- Scikit learn
- Flask
- Tableau<br><br><br>


### STRUCTURE

_Backend_<br>

Everyone - Create a basic data analysis and upload to github for EDA<br><br>

Jyotsna and Asha backend - cleanse data, modelling, creating csv for analysis -> for user recommendation, need user preference dataset<br>

- Cleaning with MongoDB(?)
- Limitization (remove special characters etc)
- Tokenization after cleaning
- Sentiment analysis - textblob and vader use natural language processing techniques
- Word frequency distribution
- Create a new file for each model
- Asha and Jyotsna can create a machine learning model and create test<br><br>

_Frontend_<br>
- Javascript, HTML, CSS<br>
- Flask API<br>
- Tableau<br>


Priya to create a dashboard, combine tableau with Javascript and connection through Flask
- Create multiple routes or single routes
- User interaction through Java, anything that needs interaction will be in the dashboard
- Combine Tableau wth Javascript and connection through Flask or using Teapot to input data<br>
- Taryn to create analysis in Tableau 
 

### TO COMPLETE 31/10
Decide on the best logic for clean csv file 
Once we have the code ready and limitized
Start tokenization
- Sentiment analysis
- Word frequency distribution
- Decide whether to create multiple routes or single routes
